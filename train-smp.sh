CUDA_VISIBLE_DEVICES=0,1,2,3 python -u -m torch.distributed.launch --nproc_per_node 2 train.py --distributed-no-spawn --no-epoch-checkpoints --keep-last-epochs 1 --keep-interval-updates 2  --seed 200 --lr 3e-5 --encoder-lr-scale 1 --decoder-lr-scale 30 --min-lr 1e-09  --max-epoch 5 --decoder-layers 2 --update-freq 4 --task bert_concat_decoder --arch transformer_bert  --token-types 15 --data /nas/qsj/data-smp/fairseq-smp-300w  --optimizer adam --adam-betas '(0.9, 0.999)'  --criterion cross_entropy  --lr-scheduler inverse_sqrt --decoder-head 12 --share-decoder-input-output-embed  --max-tokens 4000 --max-sentences 32 --tokenizer-dir /nas/qsj/bert-model/bert-base-chinese-jieba  --warmup-init-lr 1e-07 --min-lr 1e-08 --warmup-updates 4000  --no-progress-bar --weight-decay 0.01 --log-interval 1  --clip-norm 0.0  --save-interval-updates 3000  --save-dir /nas/qsj/checkpoints/SMP/fairseq-smp-300w | tee -a train-log/fairseq-smp-300w.log
